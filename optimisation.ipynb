{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7ce85707d94a1198762d6465052d4857",
     "grade": false,
     "grade_id": "cell-513963dffd022dec",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Data Fundamentals (H)\n",
    "John H. Williamson -- Session 2024/2025\n",
    "\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6/7: **Assessed**\n",
    "# Optimisation and gradient descent\n",
    "### (V13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$\\newcommand{\\vec}[1]{ {\\bf #1}} \n",
    "\\newcommand{\\real}{\\mathbb{R}}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\vec{x}\\real\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of this lab\n",
    "This lab should help you:    \n",
    "\n",
    "* understand how optimisation can be used to solve approximation problems.\n",
    "* understand how hyperparameter search and penalisation are used in optimisation\n",
    "* use optimisation to solve a simple machine learning problem.\n",
    "* use automatic differentiation to accelerate optimisation.\n",
    "* understand the carbon impacts of machine learning.\n",
    "\n",
    " <div class=\"alert alert-info\">\n",
    "    \n",
    "**This exercise is structured more like a tutorial, with more reading and experimentation than writing new code.**\n",
    "    \n",
    "In the lecture notes I used `History` to track states of an optimiser. You will not have this class available in this lab; you will have to track the states of the optimisation yourself. Do **not** try and use `History` in your solution.\n",
    "    \n",
    "</div>    \n",
    "\n",
    "\n",
    "You will need to use these functions in the latter part of the lab:\n",
    "\n",
    "* `autograd.grad` to compute gradients.\n",
    "* `autograd.flatten` to convert structured data to flat vectors.\n",
    "\n",
    "You will implement a very simple form of **deep learning** in this lab, using first-order optimisation to learn an approximating function.\n",
    "If you find the concepts difficult, you might find this video helpful: [**How machines learn**](https://www.youtube.com/watch?v=IHZwWFHWa-w).\n",
    "\n",
    "---\n",
    "\n",
    "You will need to write functions that return functions in this lab (closures), like the example below. If you've not done this before, look at the example below (or search for \"python closures\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_fn(attribute, fruit_1):\n",
    "    def describe_fruit(fruit_2, comparison):\n",
    "        # note: attribute and fruit_1 are bound\n",
    "        # into this function -- it remembers them\n",
    "        print(f\"{fruit_1}'s {attribute} is {comparison} than {fruit_2}\")\n",
    "    return describe_fruit\n",
    "\n",
    "apple_taste = make_fn(\"taste\", \"apple\") # makes a new function\n",
    "grape_shape = make_fn(\"shape\", \"grape\") # makes a new function\n",
    "apple_taste(\"banana\", \"sweeter\") # calls the new function\n",
    "apple_taste(\"hedgehog\", \"better\") # calls the new function\n",
    "grape_shape(\"pineapple\", \"smoother\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "You need to install `autograd.` The cell below will autoinstall this for you if the machine you are using does not already have it installed. **Remember to restart the kernel after packages are installed, and run this cell again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "741c085d60b84ed1c28e004a4109c2bb",
     "grade": false,
     "grade_id": "cell-1ae50fcab0c98e8b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from proxy_install import proxy_install\n",
    "\n",
    "proxy_install(\"autograd\")\n",
    "proxy_install(\"dyn_plot\")\n",
    "proxy_install(\"jhwutils\")\n",
    "\n",
    "import autograd.numpy as np\n",
    "rng = np.random.default_rng(12345)\n",
    "from autograd import grad \n",
    "from autograd.misc.flatten import flatten    \n",
    "\n",
    "# custom utils\n",
    "from jhwutils.checkarr import array_hash, check_hash\n",
    "import jhwutils.image_audio as ia\n",
    "import jhwutils.tick as tick\n",
    "np.set_printoptions(suppress=True)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(\"✅ Everything imported OK!\")\n",
    "plt.rc('figure', figsize=(8.0, 4.0), dpi=140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence graphs\n",
    "\n",
    "It's important to be able to debug how an optimisation is going. We'll use a very simple `Convergence` graph to plot the loss function as we go along. Run the example below for how it works. If you can't get the graph to work on your machine, you can call `Convergence(graph=False)` to switch to a text update.\n",
    "\n",
    "**Use Convergence() in all of your solutions (the skeleton code will usually include it for you). *Always* show the objective function's value somehow!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "from convergence import Convergence, no_graph\n",
    "\n",
    "# try Convergence(graph=False) to see the difference\n",
    "# interval=1 means it will update the plot every iteration\n",
    "# (you can set e.g. interval=100 to update every 100 iterations)\n",
    "# you can always change the interval to see more or less clearly \n",
    "# (but it will slow down the code a bit if you make it too frequent)\n",
    "c_graph = Convergence(interval=1)\n",
    "\n",
    "target = 310\n",
    "guess = 0.9 \n",
    "\n",
    "while abs(guess**2 - target)>0.0001:\n",
    "    guess = (guess + target/guess)/2\n",
    "    # just to slow it down a bit so you can see\n",
    "    # don't do this in your code!\n",
    "    time.sleep(0.1)     \n",
    "    c_graph.update(abs(guess**2 - target))\n",
    "c_graph.close() # optional, just removes extra plot\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A: Doing the washing \n",
    "\n",
    "\n",
    "You are in charge of a new machine learning startup to improve the way clothes washing is done. Automatic washing machines are one of the most important labour saving devices ever invented, and freed up people (mainly women) to do more interesting things with their lives than scrubbing dirt out of fabric. However, basic washing machines are imperfect; they consume vast quantities of energy (and thus generate large CO2 emissions) as well as fresh water, and increase pollution through generation of grey water and microplastics.\n",
    "\n",
    "How can we use **optimisation** to improve this?\n",
    "\n",
    "\n",
    "<img src=\"imgs/laundry.jpg\" width=\"40%\">\n",
    "\n",
    "*[Image credit: Toby Bochan CC-BY-NC 2.0, https://www.flickr.com/photos/tobyleah/359877499]*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1 Random search for bedding \n",
    "\n",
    "A laundry engineer has come up with a complicated mathematical model to indicate how well bedsheets get washed for a particular combination of soap concentration and temperature, on a scale from 0-10, where 10 is best. The formula is given by the code `bedding_model` below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e1054855d26e4c94ab564b340c831b6",
     "grade": false,
     "grade_id": "cell-ce53bb11a99cc944",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bedding_model(params):\n",
    "    # some crazy model of how a washing machine works\n",
    "    soap_pct, temp = params\n",
    "\n",
    "    def sm(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    s = (\n",
    "        sm((temp - 55.0) / 5.0)\n",
    "        + np.exp(-((soap_pct - 1.0) ** 2) * 9.0)\n",
    "        + 3 * sm((temp * soap_pct - 21.0) / 15.0)\n",
    "    )\n",
    "    s = (\n",
    "        s\n",
    "        + 4\n",
    "        * np.exp(-((soap_pct ** (temp * 0.015) - 1.5) ** 2) * 1.1)\n",
    "        * np.exp(-(((temp - 10) / ((soap_pct**2 + 0.212) / 4.0) - 82.5) ** 2) / 400)\n",
    "        ** 0.5\n",
    "    )\n",
    "    return s * sm((temp - 20.0) / 13.0) * 1.3 * (1 - sm((temp - 90.0) / 5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8b5c013982a3ed5d8fb7aa212a1b6a9",
     "grade": false,
     "grade_id": "cell-cdabc236bba8ee45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show the results for a few example soap percentages\n",
    "fig, ax = plt.subplots()\n",
    "soap_pct = np.linspace(0, 3, 100)\n",
    "temp = np.linspace(10, 100, 100)\n",
    "\n",
    "for i, soap_pct in enumerate([0.0, 0.25, 0.5, 1, 2, 3]):\n",
    "    ls = [\":\", \"--\", \"-.\", \"-\", \"--\", \":\"]\n",
    "    ax.plot(\n",
    "        temp, bedding_model([soap_pct, temp]), label=f\"soap={soap_pct}%\", ls=ls[i], lw=1\n",
    "    )\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Temperature (°C)\")\n",
    "ax.set_ylabel(\"Wash quality\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the best setting for soap_pct and temperature to get the best wash quality?\n",
    "\n",
    "One simple optimisation solution would be to use **random search**, iteratively evaluating $L(\\theta)$ where $\\theta$ = `[soap_pct, temperature]`. \n",
    "\n",
    "We just need to make random guesses in some feasible set and keep the best solution found so far. Write code that does this, calling your function `random_search(obj_fun, ranges, max_iter, rng)` that takes:\n",
    "\n",
    "* an objective function `obj_fun(theta)` that takes a parameter vector $\\theta$ and returns a scalar value, \n",
    "* an array `ranges` of shape $(N, 2)$ specifying the minimum and maximum value for $\\theta$ for each dimension (i.e. the feasible set),\n",
    "* runs for `max_iter` iterations.\n",
    "* It should take a random number generator `rng` as an argument. Use *uniform* random numbers (`rng.uniform`) to generate the random values. \n",
    "\n",
    "Return the best $\\theta$ found.\n",
    "\n",
    "**Write a general random_search function -- do not hardcode bedding_model! Use Convergence to show your progress**\n",
    "\n",
    "Once you have a working random search, use it to find the best washing machine `temperature` and `soap_pct` and store them in the variables `best_temperature` and `best_soap_pct`. \n",
    "\n",
    "* Remember that optimisation functions always *minimise* their objective function. \n",
    "* Assume the feasible set for the washing machine is $[0, 5]$ for soap_pct and $[0, 100]$ for temperature.\n",
    "* You should find a good setting within 20_000 iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "adda674eb3e33b7fc03eab2c9ea3e87f",
     "grade": false,
     "grade_id": "cell-b7ae3f97e9286c3d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_search(l, ranges, max_iter, rng):\n",
    "    conv = Convergence(interval=500) \n",
    "    # YOUR CODE HERE\n",
    "    conv.close()\n",
    "    return best_theta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d30de4bcc891d959046ab8b30895c257",
     "grade": true,
     "grade_id": "cell-1cfb6b5323799516",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12345)\n",
    "\n",
    "def simple_objective_function(theta):\n",
    "    return (5-(theta[0]-theta[1])**2)**2\n",
    "\n",
    "def hard_objective_function(theta):    \n",
    "    return np.sum((np.argsort(theta)-theta)**2)\n",
    "\n",
    "def test_function(theta):\n",
    "    return 1\n",
    "\n",
    "# check that the general random search function works\n",
    "with tick.marks(4):    \n",
    "    print(\"Testing random search...\")\n",
    "    with no_graph():\n",
    "        rng = np.random.default_rng(12345)\n",
    "        test_theta = random_search(test_function, np.array([[-5,5]]).reshape(-1, 2), 1, rng)        \n",
    "        assert np.allclose(test_theta, [-2.7266397], atol=1e-1), \"Are you using the passed in rng? And just making one guess per iteration?\"        \n",
    "        rng = np.random.default_rng(12345)\n",
    "        best_simple_theta = random_search(simple_objective_function, np.array([-5,5]*2).reshape(2, 2), 750, rng)\n",
    "        best_simple_loss = simple_objective_function(best_simple_theta)\n",
    "        assert best_simple_loss < 0.1, \"Are you you sure you are *minimising* the objective function?\"\n",
    "        rng = np.random.default_rng(12345)\n",
    "        best_hard_theta = random_search(hard_objective_function, np.array([0,5]*8).reshape(-1, 2), 10000, rng)\n",
    "        best_hard_loss = simple_objective_function(best_hard_theta)\n",
    "        assert best_hard_loss < 20,  \"Hmm, something is off with random search. Check your ranges and number of iterations.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e231be77c807bbc8f6ff371f5ef0273c",
     "grade": false,
     "grade_id": "cell-71069769738ba0df",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12345)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5b84eb33ae67a709d8a5e7d11f9317c",
     "grade": true,
     "grade_id": "cell-cbf2cd593d1fc0b6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tick.marks(2):\n",
    "    assert abs(best_soap_pct - 1.5) < 0.3\n",
    "    assert abs(best_temperature - 66.0) < 3\n",
    "    assert bedding_model([best_soap_pct, best_temperature]) > 9.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2 Enzymatic curves \n",
    "\n",
    "Biological washing powders use *enzymes* to break down contaminants in the wash. The activity of these enzymes is a function of the pH level (acidity) of the water. Biologists have done extensive experiments and found experimentally activity levels of enzymes across a range of pHs.\n",
    "\n",
    "We can see this collection of points, each representing one input $x$ (pH) and one output $y$ (enyzmatic activity). A washing machine might want to be able to use a model like this to predict how to release enzymes based on the current pH to get the best wash. But we can't just store hundreds of data points in the microcontroller of a washing machine. We need to find a simple *function* that approximates this data that could be stored in a small memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85939009ef128b2af25c1269477e6eb9",
     "grade": false,
     "grade_id": "cell-f5e2f32552cf306e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the raw data we are going to plot\n",
    "ph_data = np.loadtxt(\"data/ph_data.csv\", delimiter=\",\") \n",
    "ph_data[:,0] = (ph_data[:,0]-7)/7\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(ph_data[:,0]*7+7, ph_data[:,1], s=2)\n",
    "ax.set_xlabel(\"pH\")\n",
    "ax.set_ylabel(\"Enzymatic activity\")\n",
    "ax.set_title(\"pH vs Enzymatic activity\")\n",
    "ax.set_xlim(0,14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximation\n",
    "Imagine I wanted to approximate a smooth, positive curve from this data (e.g. so I could *interpolate* -- predict in between data points). How might I do it? I might decide on a curve form that is easy to compute. One simple kind of curve is given by the equation:\n",
    "\n",
    "$$f(x) = \\log \\left(1+e^ {\\left(\\sum_{i=1}^n a_i (x-a_0)^i e^{-i}\\right)}\\right)\\\\\n",
    "$$\n",
    "\n",
    "> Note: you **absolutely do not** have to understand this function, but it's a simple curve that can approximate a lot of other (positive) curves.\n",
    "\n",
    "This has a bunch of parameters $a_{0}, a_{1}, ..., a_{n-1}$ that define the curve. Adjusting those values adjusts the curve shape. If I fix some maximum number of terms $n$, I can write this function down in code, packing all of these values into one vector $\\theta=[a_0, a_1, ..., a_{n-1}]$.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ce74b7b9e9bb032116cf77b842e7182",
     "grade": false,
     "grade_id": "cell-588a1215d024cdfd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def poly_predict(x, theta):\n",
    "    \"\"\"\n",
    "    Given an array of inputs x, and a theta = [a0, a1, a2, ...],\n",
    "    return an array of predictions for each x using the formula above\n",
    "    \"\"\"    \n",
    "    y = np.zeros_like(x)    \n",
    "    # just the equation in code\n",
    "    for i, a_i in enumerate(theta[1:]): \n",
    "        y += a_i * (x-theta[0]) ** i * np.exp(-i)           \n",
    "    return np.log(1+np.exp(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show the result with some arbitrary choices for $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55c77b29fdcccf8e30cc34521763193b",
     "grade": false,
     "grade_id": "cell-fc5766044759ac46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_curve(pts, theta, title=\"\"):\n",
    "    fig, ax = plt.subplots()\n",
    "    # create points over the input range\n",
    "    xs = np.linspace(-1, 1, 200)\n",
    "    # predict the output for those points\n",
    "    y_pred = poly_predict(xs, theta)\n",
    "    ax.plot(xs*7+7, y_pred, label=\"Predicted curve\")\n",
    "    ax.scatter(pts[:,0]*7+7, pts[:,1], label=\"Data\", c='C1', s=2)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('input')\n",
    "    ax.set_ylabel('output')\n",
    "    ax.set_xlim(0, 14)\n",
    "    ax.set_ylim(0.0, 1.5)\n",
    "    ax.set_title(title)\n",
    "\n",
    "# random theta, bad fit\n",
    "theta = [0, 0.5, 0, -1, 2]\n",
    "\n",
    "show_curve(pts=ph_data, theta=theta)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down a function that would measure how good the fit of a function to a set of points is using the arithmetic mean of squared errors (differences between true and predicted). The function `mean_squared(y, y_pred)` should take:\n",
    "\n",
    "* true inputs `y`\n",
    "* predicted outputs `y_pred`\n",
    "\n",
    "This is a simple form of a **loss function**. The loss function is a measure of how well the model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a83499498a88de10204dc52ff77ef323",
     "grade": false,
     "grade_id": "cell-fb8e46c6cd58e475",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87dc1029a6be6d9aed89ec4a0058f0b7",
     "grade": true,
     "grade_id": "cell-ba1bb0568a0a1ca4",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tick.marks(2):\n",
    "    assert mean_squared(np.array([1,2,3]), np.array([1,2,3])) == 0\n",
    "    assert mean_squared(np.array([1,2,3]), np.array([4,5,6])) == 9\n",
    "    assert mean_squared(np.array([3,2,1]), np.array([4,5,6])) == 11.66666666666666667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could measure how good a particular $\\theta$ is by measuring how much error there was predicting the $y'=f(x)$ of the data points from the inputs $x$ (each of which is an $x,y$ pair) using this loss function. \n",
    "\n",
    "We want to find the $\\theta$ that minimises this loss function. Write a function `approximation_loss` that takes:\n",
    "\n",
    "* a set of data points `x` and `y`\n",
    "* a loss function `loss_fn`\n",
    "* a prediction function `f`\n",
    "\n",
    "and returns a **new function** that takes *only* a parameter vector $\\theta$ and returns the loss. **Note: you need to use a nested function or a lambda expression to do this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14b9a8dddec41be38d23b66e622a8360",
     "grade": false,
     "grade_id": "cell-65be12fa37221d65",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def approximation_loss(x, y, f, loss):\n",
    "    # YOUR CODE HERE\n",
    "    return loss_fn     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b3e74e4ba2a1f13fdb2c94f7429060f",
     "grade": true,
     "grade_id": "cell-b9a5b1f95b1ac7a3",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tick.marks(2):\n",
    "    my_loss = approximation_loss(ph_data[:,0], ph_data[:,1], poly_predict, mean_squared)\n",
    "    assert callable(my_loss), \"You need to return a function\"\n",
    "    stupid_loss = approximation_loss(np.array([1,2,3]), np.array([1,2,3]), lambda x,theta: x, mean_squared)\n",
    "    assert stupid_loss([]) == 0, \"You need to use the loss function\"\n",
    "    stupid_loss_2 = approximation_loss(np.array([1,2,3]), np.array([4,5,6]), lambda x,theta: x, mean_squared)\n",
    "    assert stupid_loss_2([]) == 9, \"You need to use the loss function\"\n",
    "    test_loss = approximation_loss(np.array([1,2,3]), np.array([4,5,6]), lambda x,theta: x+theta, mean_squared)\n",
    "    assert test_loss([3,3,0]) == 3, \"Are you calling f?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could find a curve that fits our data by minimising this loss function using `random_search`. Find a curve using a $\\theta$ of length 4 (i.e. $\\theta=[a_0, a_1, a_2, a_3]$) and store the best parameters as `best_theta`. \n",
    "\n",
    "Use a range of [-5, 5] for the range of guess for each of the entries in $\\theta$.\n",
    "\n",
    "Note: the result will not be very good. Use a max of 10000 iterations to find the best $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6fb5fe426fdcfe72515dde1c1d3e277a",
     "grade": false,
     "grade_id": "cell-5cfac61e3f6fa81a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12345)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf80f3a3b987d7e209479f8fb3166f39",
     "grade": true,
     "grade_id": "cell-73fba38c087b8a18",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_curve(pts=ph_data, theta=best_theta, title=\"Fitted curve\")\n",
    "with tick.marks(4):\n",
    "    loss = approximation_loss(ph_data[:,0], ph_data[:,1], poly_predict, mean_squared)(best_theta)\n",
    "    print(loss)\n",
    "    assert loss < 0.3, \"Are you sure you are minimising the loss?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.3 Hill-climbing\n",
    "\n",
    "Random search doesn't assume that parameters are continuous-valued or that the objective function is smooth. Therefore, it is not a particularly good optimisation algorithm for problems where these assumptions hold. Stochastic hill-climbing allows for a more efficient search by making \"local\" movements in parameter space.\n",
    "\n",
    "The code below implements hill-climbing for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c6611469363a6f09316b457b0764573",
     "grade": false,
     "grade_id": "cell-a7417927fb83bf81",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_init_fn(limits, rng):\n",
    "    # this function returns a function init_fn with a fixed range\n",
    "    return lambda: rng.uniform(limits[:,0], limits[:,1])\n",
    "\n",
    "def get_proposal_fn(delta, rng):    \n",
    "    # this function returns a function proposal_fn with a fixed delta\n",
    "    return lambda x: x+rng.normal(0, delta, x.shape)\n",
    "    \n",
    "def hill_climbing(l, max_iters, init_fn, proposal_fn):    \n",
    "    best_guess = init_fn()    \n",
    "    best_loss = l(best_guess) # work out how bad it is    \n",
    "    c = Convergence(interval=1000) # you can change this <--\n",
    "    for i in range(max_iters):                \n",
    "        proposed = proposal_fn(best_guess)   \n",
    "        proposed_loss = l(proposed)\n",
    "        # check if we beat the record\n",
    "        if proposed_loss<best_loss:\n",
    "            best_guess, best_loss = proposed, proposed_loss            \n",
    "        c.update(best_loss)\n",
    "    c.close()\n",
    "    return best_guess\n",
    "\n",
    "# example usage\n",
    "# dimensions = 6\n",
    "# limits = [-5, 5]\n",
    "# delta = 0.1\n",
    "# hill_climbing(my_loss, 10_000, get_init_fn(limits=np.array([limits]*dimensions), rng=rng), get_proposal_fn(delta=delta, rng=rng))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply hill-climbing to the problem of finding the best curve to fit the data, using a **8 dimensional** $\\theta$ (again, all values initially in range [-5, 5]). Use a maximum of 20_000 iterations. You will need to adjust the hyperparameter `delta` (step size) to get good results. Store the best $\\theta$ in the variable `best_theta` and the best loss in the variable `best_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "494bc9b6742145f22b0f4c383ec0bb58",
     "grade": false,
     "grade_id": "cell-7f7aa2b28dbcd833",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12345)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4aa99890580f04125dca876e8e557e1",
     "grade": true,
     "grade_id": "cell-ed3297d9573be2b9",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_curve(ph_data, best_theta, title=f\"Hill climbing: Loss {best_loss:.2f}\")\n",
    "plt.show()\n",
    "\n",
    "with tick.marks(4):\n",
    "    assert approximation_loss(ph_data[:,0], ph_data[:,1], poly_predict, mean_squared)(best_theta) == best_loss\n",
    "    assert best_loss < 0.04, \"Hmm, you might need to adjust delta or the number of iterations\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.4 Annealing the motor parameters\n",
    "\n",
    "\n",
    "<img src=\"imgs/motor.jpg\" width=30%>\n",
    "\n",
    "*[Image credit: Hausmann, https://www.deviantart.com/hausmann/art/Electric-Motor-Final-Cross-section-665887814, CC-NC-ND-3.0]* \n",
    "\n",
    "You are asked to help optimise the *motors* inside a washing machine. These motors have 50 distinct parameters that can be adjusted to change the timing of the way motor elements are energised. The motors are very sensitive to the exact settings of these parameters, and the engineers have found that the motors work poorly if the settings are not just right. Unfortunately, there are 50 parameters and all of the parameters interact in a complex way.\n",
    "\n",
    "Fitting a smooth curve to points is relatively easy in optimisation terms; the resulting objective function is smooth and **convex**. This means that there is only one minimum, and it is easy to find by \"walking down the hill\". But nonconvex functions like this particular motor problem have many minima. It has many \"pockets\" which will trap local optimisers, and in high dimensions, it is very hard to find the global minimum just by guessing.\n",
    "\n",
    "A 2D slice of the \"motor parameter\" objective function that the engineers are trying to optimise is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9976bbfce405f612e3931036c539a1c",
     "grade": false,
     "grade_id": "cell-ba4ce18014c21446",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nasty objective function\n",
    "def motor_model(x):\n",
    "    x = x - np.arange(x.shape[-1])*0.5\n",
    "    return  np.linalg.norm((x**2 - 19*np.cos(1.5*x+np.sin(x+1.2))), axis=-1, ord=2) \n",
    "grid = np.linspace(-7,7, 120)\n",
    "mx, my = np.meshgrid(grid, grid)\n",
    "mz = motor_model(np.stack([mx, my]).T)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.contourf(mx, my, mz, levels=25, cmap='magma')\n",
    "ax.set_xlabel(\"Motor param 1\")\n",
    "ax.set_ylabel(\"Motor param 2\")\n",
    "ax.set_aspect(1.0)\n",
    "plt.show()\n",
    "\n",
    "# show it in 3D\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d', proj_type = 'persp')\n",
    "ax.set_xlabel(\"Motor param 1\")\n",
    "ax.set_ylabel(\"Motor param 2\")\n",
    "\n",
    "\n",
    "ax.plot_surface(mx, my, mz, color='w', rstride=2, cstride=2)\n",
    "# adjust elevation\n",
    "ax.view_init(50, 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither hill-climbing nor random search will be able to find good solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try and solve it in 50 dimensions with hill climbing\n",
    "# it will plateau at a local minimum\n",
    "rng = np.random.default_rng(12345)\n",
    "\n",
    "init_fn = get_init_fn(limits=np.array([-50,50]*50).reshape(-1, 2), rng=rng)\n",
    "proposal_fn = get_proposal_fn(delta=0.2, rng=rng)\n",
    "theta = hill_climbing(motor_model, 30_000, init_fn, proposal_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also very ineffective, dimension is too high\n",
    "rng = np.random.default_rng(12345)\n",
    "theta = random_search(motor_model, np.array([-50,50]*50).reshape(-1, 2), 30_000, rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**\n",
    "\n",
    "Create a modified version of `hill_climbing` called `annealed_climbing` which implements **simulated annealing**. \n",
    "\n",
    "In simulated annealing, a temperature $T$ is used to control the probability of accepting a worse solution (i.e. an uphill step). \n",
    "\n",
    "* Instead of only accepting steps that go \"downhill\", the probability of accepting a new step is given by $p = \\exp(-\\Delta E / T)$\n",
    "* $\\Delta E$ is the difference in objective function's value between the current and proposed parameter vectors. \n",
    "* The temperature $T$ starts at a defined level $T_0$ is reduced on each iteration by a factor $\\alpha$, where $\\alpha = 1-e^{-T_d}$.\n",
    "* You can make a decision with probability $p$ by generating a uniform random number $r$ in [0,1] and accepting if $r < p$.\n",
    "\n",
    "Use `annealed_climbing` to find a good setting for the motor problem in the true 50D space (the code in the test below will do this for you, if you implement `annealed_climbing` correctly). \n",
    "\n",
    "**You will need to experiment to choose a good value for the hyperparameters $T_d$ and $T_0$ to get good results. $T_d$ should be in the range [1, 20], and $T_0$ should be in the range [1, 5000]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba8e1477728278b86ac5dee567b6c3d9",
     "grade": false,
     "grade_id": "cell-d7c3e3611d918a66",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def annealed_climbing(l,  max_iters, init_fn, proposal_fn, T_d, T_0, rng):    \n",
    "    \n",
    "    c = Convergence(interval=1000)   \n",
    "    alpha = 1 - np.exp(-T_d)\n",
    "    T = T_0\n",
    "    # YOUR CODE HERE\n",
    "    c.close()\n",
    "    return best_guess\n",
    "\n",
    "# Change these\n",
    "# T_d, T_0 = 50, 0.5\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0c08bc4ee18aa3e46aeda8a7454413d",
     "grade": true,
     "grade_id": "cell-3e905b11144d1de3",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tick.marks(4):\n",
    "    rng = np.random.default_rng(12345)\n",
    "    \n",
    "    init_fn = get_init_fn(limits=np.array([-50,50]*50).reshape(-1, 2), rng=rng)\n",
    "    proposal_fn = get_proposal_fn(delta=0.22, rng=rng)    \n",
    "\n",
    "    with no_graph():        \n",
    "        theta = annealed_climbing(motor_model, 4_000, init_fn, proposal_fn, T_d=1000, T_0=1000000.0, rng=rng)    \n",
    "        assert motor_model(theta) > 500.0\n",
    "    \n",
    "    # this code will search for a solution to the motor problem in 50D\n",
    "    theta = annealed_climbing(motor_model, 30_000, init_fn, proposal_fn, T_d=T_d, T_0=T_0, rng=rng)\n",
    "    assert motor_model(theta) < 150.0 and motor_model(theta) > 0.1\n",
    "    \n",
    "print(f\"Achieved loss of {motor_model(theta):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.5 The environmental cost of machine learning\n",
    "\n",
    "Washing is one of the main contributors to CO2 production. **Approximately 20% of the UK's total CO2 emissions arise from heating water for washing clothes**. \n",
    "\n",
    "The [ONS](https://www.theguardian.com/environment/2024/sep/21/uk-public-washing-their-clothes-too-often-says-major-laundry-brand#:~:text=Figures%20from%20the%20Office%20for,260%20wash%20loads%20a%20year.) reports that the UK washes approximately 2.6 billion loads of washing per year. An intelligent washing machine could radically reduce the temperature to get equally clean clothes, for example by using optimised enzyme release timing or more efficient motor control.\n",
    "\n",
    "However, machine learning is itself a significant contributor to CO2 emissions. The training of machine learning models is a computationally intensive process that can consume vast amounts of energy. How would we quantify how much optimisation is worth doing to bring down the CO2 emissions of washing? (this is a genuinely important question to understand!)\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"imgs/coal_plants.jpg\"> \n",
    "\n",
    "*[Photo by: Robert S. Donovan (CC-BY-NC 2.0)]*\n",
    "\n",
    "---\n",
    "\n",
    "The hyperparamater optimisation we've seen is a bit slow, but at least you can run it on a standard laptop. Production machine learning models can consume vast amounts of computation during hyperparameter optimisation. It is not uncommon for a single training run to take weeks of computation on clusters of tens of thousands of GPUs. Running the computational infrastructure contributes to CO2 emissions and thereby accelerates global heating. \n",
    "\n",
    "**How can we quantify the amount of CO2 emitted during machine learning model training? One estimation and comparison tool is provided at [https://mlco2.github.io/impact/](https://mlco2.github.io/impact/).**\n",
    "\n",
    "Use the link provided above to estimate the kg CO2 emitted during optimisation under the following assumptions. These assumptions are realistic for big ML training runs undertaken by companies like Google, Facebook, and Amazon.\n",
    "\n",
    "---\n",
    "* hyperparameter optimisation is carried out using **grid search**\n",
    "* one optimisation run takes 2 days on 8 GPU cluster of RTXA6000s\n",
    "* 4096 optimisation runs are needed to complete a hyperparameter grid search of 4 parameters divided into 8 values each (4096 = $8^4$)\n",
    "* `Amazon Web Services` is used as the cloud provider\n",
    "* Compute instances are based in the `Canada (Central)` region\n",
    "* *ignore any reported offsetting*\n",
    "---\n",
    "\n",
    "* Store the estimated CO2 emissions for this hyperparameter optimisation process in `co2_canada`\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "* Evaluate the CO2 impact of performing this computation instead with `CoreWeave` in the `US Central` region (store as `co2_coreweave`) and store the increase in CO2 from AWS Canada to this service in `co2_diff`.\n",
    "\n",
    "--- \n",
    "\n",
    "* Assume one tree absorbs 22 kg of CO2 per year (this is extremely approximate -- there's not a \"standard tree\"!). How many **more** trees would need to be planted to offset the CO2 emissions from the optimisation when run on CoreWeave's service than on AWS Canada? Store this in `wasted_trees`\n",
    "\n",
    "--- \n",
    "\n",
    "* Assume you have four hyperparameters and you apply **grid search** to find the best hyperparameter settings. For the AWS Canada run above, if you instead had seven hyperparameters (again divided into eight values each, and again on the AWS Canada server) how many **more** trees would need to be planted to offset the increase CO2 emissions from the finer grid search? Store this in `wasted_trees_grid`. Assume nothing else affects the time taken to run the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a76691e2714d0af06c6ec3f3905c56c",
     "grade": false,
     "grade_id": "cell-a12014273c1b3224",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c4224ad2bf069806c37094b88abd33e",
     "grade": true,
     "grade_id": "cell-e974d8fcf8303ffe",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tick.marks(2):\n",
    "    assert(check_hash(np.array(co2_canada, dtype=int), ((), 47185.0)))\n",
    "    print(f\"The Canada compute run would have produced {co2_canada} kg of CO2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74bf47d3f17aea1c3c8808a28c67b2ac",
     "grade": true,
     "grade_id": "cell-b407925ab876e6b3",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tick.marks(2):\n",
    "    print(f\"The CoreWeave (US) compute run would have produced {co2_coreweave:.0f} kg of CO2\")\n",
    "    assert(check_hash(np.array(co2_coreweave, dtype=int), ((), 990900.0  )))    \n",
    "    assert(check_hash(np.array(wasted_trees, dtype=float), ((), 42896.2909090909  )))\n",
    "    print(f\"You would need an extra {wasted_trees:.0f} trees to absorb this much carbon compared to AWS (Canada), approximately {wasted_trees/30e6:.3%} of Scotland's total annual plantation.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd4f81c8a39963be60045d41057cd977",
     "grade": true,
     "grade_id": "cell-296b836adff25cf9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tick.marks(2):    \n",
    "    print(f\"The move from four to seven hyperparameters in the grid search would take {wasted_trees_grid:.0f} trees to absorb the extra carbon, approximately {wasted_trees_grid/30e6:.3%} of Scotland's total annual plantation.\")\n",
    "    assert(check_hash(np.array(wasted_trees_grid, dtype=float), ((), 1095999.7681818183)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the CO2 cost of running a load of washing in a standard washing machine at different temperatures. Imagine we are using the hyperparameter optimisation to find the best settings for a new washing machine that reduces the temperature needed to get a \"good\" wash. \n",
    "\n",
    "Assume the following facts:\n",
    "\n",
    "* Someone has calculated how much expected reduction in temperature we can expect from each hyperparameter evaluation made (of course, we don't usually have good estimates of this, but let's pretend we do). The function `estimated_temp_reduction(hyperparam_evals)` (below) estimates the average reduction in washing temperature that could be expected after running `hyperparam_evals` evaluations of hyperparameters.\n",
    "* 2.6 billion washes are done per year in the UK.\n",
    "* The average temperature of a wash is currently 41.2C.\n",
    "* The optimisation evaluations are done on AWS Canada.\n",
    "\n",
    "---\n",
    "\n",
    "What is the maximum number of hyperparameter evaluations that would be viable to reduce carbon emissions in the UK over the course of a year? Specifically, what is the largest number of optimization runs that could be performed where the CO2 emissions from these evaluations remain lower than the annual CO2 emissions saved by the improved average temperature reduction?\n",
    "\n",
    "Store the answer in `viable_runs`. Round to the nearest 1000.\n",
    "\n",
    "How many divisions of the *four hyperparameter* grid search  does this correspond to (rounding to the nearest integer)? Store this in `viable_divisions`.\n",
    "\n",
    "*Note: this is an optimisation problem! Hint: The objective function has the form: minimise |f(a)-g(a)|. Make sure you start with initial guesses in a suitable range (e.g. billions of hyperparameter runs)*\n",
    "\n",
    "*If you get stuck, try plotting a graph to see the shape of the objective function before optimising*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of CO2\n",
    "# Note: it's nearly impossible to find exact numbers for all of these\n",
    "# but this is pretty close to the average CO2 equivalent consumption \n",
    "# for a standard UK 5kg washing machine load (at least for 30,40,60,90)\n",
    "\n",
    "def washing_co2(temp):\n",
    "    return 0.1 + ((temp - 20)/82.0 + (temp-20)**2/6000.0) \n",
    "\n",
    "def estimated_temp_reduction(hyperparams_evals):\n",
    "     def sigmoid(x):\n",
    "          return 1/(1+np.exp(-x))\n",
    "     return sigmoid(hyperparams_evals/1e5)*0.25 + np.exp(-hyperparams_evals-10)\n",
    "\n",
    "# show graphs of the CO2 usage and the estimated temperature reduction\n",
    "washing_temperature_c = np.array([20, 30, 40, 50, 60, 70, 80, 90])\n",
    "co2_usage_kg = washing_co2(washing_temperature_c)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(washing_temperature_c, co2_usage_kg, label=\"CO2 usage (kg)\")\n",
    "ax.set_xlabel(\"Washing temperature (°C)\")\n",
    "ax.set_ylabel(\"CO2 usage (kg)\")\n",
    "ax.set_title(\"CO2 usage vs Washing temperature per load\")\n",
    "ax.set_ylim(0, 2)\n",
    "ax.set_xlim(20, 90)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "hypers = np.linspace(0,1_000_000,100)\n",
    "ax.plot(hypers, estimated_temp_reduction(hypers), label=\"Estimated reduction in temperature\")\n",
    "ax.set_xlabel(\"Hyperparameter evaluations\")\n",
    "ax.set_ylabel(\"Estimated temperature reduction (°C)\")\n",
    "ax.set_title(\"Estimated temperature reduction vs Hyperparameter evaluations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91d56b9bf6634218f9cef997763dfedb",
     "grade": false,
     "grade_id": "cell-133c7c37d4c38278",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12345)\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4c1aa759d9e44af2931d57c7da720b9",
     "grade": true,
     "grade_id": "cell-f809f29af757b384",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"It would be worth doing up to {viable_runs:.0f} optimisations, or {viable_divisions:.0f} divisions of the 4 parameter grid search.\")\n",
    "with tick.marks(6):\n",
    "    assert check_hash(np.array(viable_runs), ((), 27110000.0   )), \"Make sure you are minimising differences. Also, you may have converged to a local minimum; try larger initial guesses\"\n",
    "    assert check_hash(np.array(viable_divisions), ((), 240.0 ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.6 Gradients and penalisation\n",
    "\n",
    "**Note: this part uses ideas from Week 7. You can do the first parts of Part B without Week 7, if you want to skip this part for now.**\n",
    "\n",
    "Let's return to optimising the wash temperature and soap concentration. The function `extended_bedding_model` below is an extended model of linen washing that includes two extra parameters: how long the wash runs for, and how much enzymatic content there is in the soap.\n",
    "\n",
    "We can do much better than the random search we used in part A.1, which won't work well in four dimensions. We can use **gradient descent** to find the best solution. The `autograd` package gives us a function called `grad(f)` that takes a function `f` and returns a new function that computes the gradient of `f` with respect to its first argument. \n",
    "\n",
    "Use this to implement gradient descent: $$\\theta_{t+1} = \\theta_t - \\delta \\nabla L(\\theta_t)$$\n",
    "\n",
    "where $\\delta$ is the step size (also called the \"learning rate\").  Again, write a *general* function `gradient_descent(obj_fun, init_theta, max_iter, delta)` that takes:\n",
    "\n",
    "* an objective function `obj_fun(theta)` that takes a parameter vector $\\theta$ and returns a scalar value,\n",
    "* an initial guess `init_theta`,\n",
    "* runs for `max_iter` iterations,\n",
    "* a learning rate `delta`.\n",
    "\n",
    "Return the best $\\theta$ found (as in all of the optimisers).\n",
    "\n",
    "Estimate the best washing parameters, and store the result as `best_wash_params`. Start with a fixed guess of $\\theta=[1, 30, 10, 5]$ and use a learning rate of 0.1. \n",
    "\n",
    "**Note: if you get `nan` you are either maximising where you should be minimising, or you have too large a learning rate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extended_bedding_model(params):\n",
    "    # some crazy model of how a washing machine works\n",
    "    def sm(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    soap_pct, temp, run_time, enzyme = params \n",
    "    soap_pct = soap_pct ** 2\n",
    "    enzyme_activity = np.exp(-(temp-37)**2/1000.0)\n",
    "    soap_pct = soap_pct + sm((enzyme_activity*enzyme-20)/20.0)    \n",
    "    \n",
    "    s = (\n",
    "        sm((temp - 55.0) / 5.0)\n",
    "        + np.exp(-((soap_pct - 1.0) ** 2) * 9.0)\n",
    "        + 3 * sm((temp * soap_pct  - 21.0) / 15.0)\n",
    "    )    \n",
    "    s = (\n",
    "        s\n",
    "        + 4\n",
    "        * np.exp(-((soap_pct ** np.abs(temp * 0.015) - 1.5) ** 2) * 1.1)\n",
    "        * np.exp(-(((temp - 10) / ((soap_pct**2 + 0.212) / 4.0) - 82.5) ** 2) / 400)\n",
    "        \n",
    "    )\n",
    "    s = s * sm((temp - 20.0) / 13.0) * 1.3 * (1 - sm((temp - 90.0) / 5.0))\n",
    "  \n",
    "    s = s * sm((run_time * temp * run_time  - 30000.0) / 9000.0)\n",
    "    s += np.exp(-run_time*2.0) \n",
    "    return s\n",
    "\n",
    "# Run this to see how it performs\n",
    "fig, ax = plt.subplots(ncols=6, nrows=4, figsize=(24,12))\n",
    "\n",
    "temp = np.linspace(10, 100, 100)\n",
    "for k, enzyme in enumerate([0, 10, 20, 30]):\n",
    "    for j, run_time in enumerate([0, 10, 20, 30, 40, 50]):\n",
    "        for i, soap_pct in enumerate([0.0, 0.25, 0.5, 1, 2, 3]):\n",
    "            ax[k,j].plot(\n",
    "                temp,\n",
    "                extended_bedding_model([soap_pct, temp, run_time, enzyme]),\n",
    "                label=f\"soap={soap_pct}%\",\n",
    "        )    \n",
    "        ax[k,j].set_ylim(0, 10)\n",
    "        ax[k,j].set_title(f\"{run_time} minutes, {enzyme} enzymes\")\n",
    "        ax[k,j].set_xlabel(\"Temperature (°C)\")\n",
    "        ax[k,j].set_ylabel(\"Wash quality\")\n",
    "plt.tight_layout()        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db52e12468464db7e91a93e41bd34810",
     "grade": false,
     "grade_id": "cell-b32c96aba4fd742c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_descent(obj_fun, init_theta, max_iter, delta):\n",
    "    c = Convergence(interval=1000)\n",
    "    # YOUR CODE HERE\n",
    "    c.close()\n",
    "    return theta\n",
    "\n",
    "\n",
    "best_wash_params = gradient_descent(lambda theta:-extended_bedding_model(theta), np.array([1, 30.0, 10, 5]), 20_000, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "363847d6f949e518c5e7d210cd11ec33",
     "grade": true,
     "grade_id": "cell-8249305481d187f7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tick.marks(2):\n",
    "    # check gradient descent works \n",
    "    with no_graph():\n",
    "        simple_quadratic = lambda x: (x-3)**2\n",
    "        q_theta = gradient_descent(simple_quadratic, 0.0, 100, 0.1)\n",
    "        assert np.allclose(q_theta, 3.0, atol=1e-2), \"Are you sure you are descending the gradient?\"\n",
    "        assert np.abs(simple_quadratic(q_theta))<1e-2, \"Are you sure you are minimising the function?\"\n",
    "        vector_quadratic = lambda x: np.sum((x-3)**2)\n",
    "        q_theta = gradient_descent(vector_quadratic, np.array([0.0, 0.0]), 100, 0.1)\n",
    "        assert np.allclose(q_theta, [3.0, 3.0], atol=1e-2), \"Are you sure you are descending the gradient?\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07622bf757a2d8549002bb6daac8b88c",
     "grade": true,
     "grade_id": "cell-133355280402c5b3",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tick.marks(2):\n",
    "    assert extended_bedding_model(best_wash_params) > 4.5\n",
    "    print(f\"The best washing parameters are {best_wash_params[0]:.2f}% soap, {best_wash_params[1]:.0f}°C, {best_wash_params[2]:.0f} minutes, {best_wash_params[3]:.2f}% enzymes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "A new environmental directive has been introduced that requires that the product of the run time (in minutes) and the temperature (in degrees) in a wash cycle of any new washing machine must be less than 1600. \n",
    "\n",
    "Implement a penalty function that penalises parameters that do not meet this requirement. That is: $L'(\\theta) = L(\\theta) + \\lambda C(\\theta),$ where $C(\\theta)$ is your penalty function. Call your penalty function `co2_penalty(theta)`.\n",
    "\n",
    "The penalty function should return increasingly large values as the constraint is approached. Include this penalty in your objective function and use gradient descent to find a solution that meets this environmental directive. Store the result as `best_wash_params_penalty`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7bbed8282cd6408238c29229a9142f4",
     "grade": false,
     "grade_id": "cell-1e33a5c5f2dcb36c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66fd434d3fd4f975181e247258963686",
     "grade": true,
     "grade_id": "cell-80f1d99713b9c7ed",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for temp in [10, 20, 30, 40, 50, 60, 70, 80, 90]:\n",
    "    run_time = np.linspace(0, 100, 100)\n",
    "    ax.plot(run_time*temp, co2_penalty([1, temp, run_time, 1]))\n",
    "    ax.set_ylim(-0.5, 10)\n",
    "    ax.set_xlim(0, 3000)\n",
    "    ax.set_xlabel(\"Product of temperature and run time\")\n",
    "    ax.set_ylabel(\"CO2 penalty\")\n",
    "\n",
    "with tick.marks(2):\n",
    "    assert co2_penalty([1, 30, 10, 1]) == co2_penalty([5, 30, 10,2]), \"The penalty should only depend on the product of temp and run time\"\n",
    "    assert co2_penalty([1, 30, 10,1 ]) < co2_penalty([1, 50, 50,1]), \"Hmm, the penalty does not seem to be enforced\"\n",
    "    assert co2_penalty([1, 35, 50,1]) < co2_penalty([0, 50, 50,1]), \"Hmm, the penalty does not seem to be enforced\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b84612ebcc92f00828a68dc07acc3b1",
     "grade": true,
     "grade_id": "cell-a51e2bb689f48bd3",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tick.marks(3):\n",
    "    print(f\"The best washing parameters are {best_wash_params_penalty[0]:.2f}% soap, {best_wash_params_penalty[1]:.0f}°C, {best_wash_params_penalty[2]:.0f} minutes, {best_wash_params_penalty[3]:.2f}% enzymes\")\n",
    "    assert extended_bedding_model(best_wash_params_penalty) > 3.3\n",
    "    assert best_wash_params_penalty[1] * best_wash_params_penalty[2] < 1700, \"The CO2 penalty is not being enforced well\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B: Sorting the washing\n",
    "\n",
    "Sorting the washing (\"laundry\" in American-speak) is annoying. How would we make progress towards a robot that could do it for us? \n",
    "\n",
    "<img src=\"imgs/laundry.webp\" width=\"400px\">\n",
    "\n",
    "*[Our glorious future. Credit: DALL-E.]*\n",
    "\n",
    "*You know that locked door at the front of the Boyd Orr lab, beside the visualiser? There really is a giant laundry sorting robot hiding in there.*\n",
    "\n",
    "One of the first steps in building a laundry-sorting robot is to categorize different types of clothes by looking at them. How could optimization help with this? We'll need a function that takes an image of clothing and sorts it into a category based on its features. Importantly, this function should be parameterizable—meaning we can adjust its parameters to improve how it categorizes items.\n",
    "\n",
    "#### Approximation\n",
    "We will be trying to approximate a function. This means we have an objective function of the form:\n",
    "\n",
    "$$L(\\theta) = \\|f(\\vec{x};\\theta)-y\\|$$\n",
    "\n",
    "where we measure the difference between a predicted output $f(\\vec{x};\\theta)$ and an expected output $y$, and try and minimise that difference by choosing a good setting for $\\theta$.\n",
    "\n",
    "We will build a simple \"deep learning\" system. We will completely ignore many of the important problems in machine learning, like overfitting, regularisation, efficient network architectures and fair evaluation, and concentrate on using first-order optimisation (gradient descent) to find a function that approximates this categorisation.\n",
    "\n",
    "### Classifier \n",
    "We will be approximating a classification function -- that is, we're going to ascribe a discrete label to an input image. We will be using a simple neural network to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.1 Prepare the data\n",
    "\n",
    "### Loading the data\n",
    "There are 4096 tiny images of articles of clothing in `data/fashion_train.png`. Each image is 28x28 pixels. The code below loads them. \n",
    "\n",
    "There are also a set of **labels**, which indicates the kind of article of clothing each is, as a number from 0-9.\n",
    "\n",
    "* 0 T-shirt/top\n",
    "* 1 Trouser\n",
    "* 2 Pullover\n",
    "* 3 Dress\n",
    "* 4 Coat\n",
    "* 5 Sandal\n",
    "* 6 Shirt\n",
    "* 7 Sneaker\n",
    "* 8 Bag\n",
    "* 9 Ankle boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1516e769e3a38161b8c52e058bf229fb",
     "grade": false,
     "grade_id": "cell-195050467bc084ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "clothes_image = ia.load_image_colour('data/fashion_train.png')\n",
    "clothes_labels = np.loadtxt('data/fashion_train_labels.txt', dtype=int)\n",
    "ia.show_image(clothes_image, width=\"50%\")\n",
    "print(clothes_image.shape) # rows, columns, channels\n",
    "print(clothes_labels.shape)\n",
    "label_names = [\"tshirt\", \"trouser\", \"pullover\", \"dress\", \"coat\", \"sandal\", \"shirt\", \"sneaker\", \"bag\", \"boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to transform them into a more helpful format:\n",
    "\n",
    "* Use only the *second* color channel of the image.\n",
    "* Rearrange this into a `(4096, 28, 28)` tensor. Hint: you need to reshape and then permute the axes.\n",
    "* Store the result in `clothes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1125c2ae64ad1a2b97438a2e8bd41383",
     "grade": false,
     "grade_id": "cell-437c3324471e5b99",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebe557d8b48f59e6dfcbb493ebce35e7",
     "grade": true,
     "grade_id": "cell-93d25e17266b10fc",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with tick.marks(4):\n",
    "    assert clothes.shape==(4096, 28, 28)\n",
    "    assert np.max(clothes)<=1.0\n",
    "    assert np.min(clothes)>=0.0\n",
    "    assert np.mean(clothes) > 0.1\n",
    "    cl = clothes.reshape(64, 64, 28, 28).swapaxes(0,1).reshape(4096, 28, 28)     \n",
    "    assert not array_hash(cl) == ((4096, 28, 28), 1468618882187.8767 ), \"You've got the (big) rows and columns the wrong way around\"\n",
    "    assert check_hash(clothes, ((4096, 28, 28),1468618882187.8767  ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c786d7d7f3e02d55",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You'll see a selection of low-res images below if your code is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "578509400001bc2541549809b7d3b591",
     "grade": false,
     "grade_id": "cell-55ee812835b11591",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_cloth(cloth, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.imshow(cloth, cmap='bone')\n",
    "    ax.axis('off')\n",
    "\n",
    "rng = np.random.default_rng(12345)\n",
    "fig, _ = plt.subplots(4, 5, figsize=(4,3))\n",
    "for i in range(20):\n",
    "    ax = plt.subplot(4, 5, i+1)\n",
    "    idx = rng.choice(clothes.shape[0])  \n",
    "    show_cloth(clothes[idx], ax=ax)\n",
    "    fig.set_facecolor('black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A deep network\n",
    "How will this parameterisable function that maps from images to orientations be defined? We will use a very simple deep \"neural network\" predictor. This is an incredibly simple algorithm. It takes an input vector, then repeatedly:\n",
    "\n",
    "* applies a fixed elementwise non-linearity; this allows the network to learn non-linear functions;\n",
    "* multiplies the vector by a matrix and adds a vector. Traditionally, these are called the \"weights\" and \"biases\" of the network, but it's just $\\vec{y} = W\\vec{x} + \\vec{b}$\n",
    "\n",
    "We have to define the *shape* of each of the matrices which will be used to transform the vector, but we *optimise* to find the elements that go into those matrices. This is the \"learning\" part.\n",
    "\n",
    "Each of these steps is traditionally called a \"layer\" of the prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9fac38367e9c289090276275bfdfabc",
     "grade": false,
     "grade_id": "cell-a9db747a45096704",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a very basic neural network\n",
    "def swish(x):\n",
    "    return x * (1.0/(1.0 + np.exp(-x)))\n",
    "\n",
    "def nn_predict(theta, x, unflatten):     \n",
    "    # for each layer   \n",
    "    for w,b in unflatten(theta):         \n",
    "        x = w.T @ swish(x) + b\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.2 Making vectors\n",
    "\n",
    "We have to be able to write this problem in the form:\n",
    "\n",
    "$$L(\\theta) = \\|f(\\vec{x};\\theta)-\\vec{y}\\|$$\n",
    "\n",
    "Every input vector $\\vec{x}$ must have a corresponding matched expected output $\\vec{y}$, and we need a function $f$ that depends on $\\vec{x}$ and $\\theta$. **All of these things ($x,y,\\theta$) have to be made into vectors**.\n",
    "\n",
    "### B.2.1 Input vectors: unraveling images\n",
    "What is $\\vec{x}$? How can we define the input to this function? We need to have one **vector** per example; that is a matrix with one row per face image. We can do this by reshaping the `clothes` tensor to unravel the 28x28 pixel image into a single 784 dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** \n",
    "\n",
    "Reshape the clothes tensor to a (4096,784) matrix; each row being a clothes image as a single \"unravelled\" vector. Store this in `clothes_inputs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9b9f81be62f38986463d73af06438c6",
     "grade": false,
     "grade_id": "cell-103275cde9c46f8a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58a8592fa65d263ccca003616621d4ef",
     "grade": true,
     "grade_id": "cell-ac865ff0662dc92e",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with tick.marks(4):\n",
    "    assert clothes_inputs.shape == (4096, 784)\n",
    "    print(array_hash(clothes_inputs))\n",
    "    assert(check_hash(clothes_inputs,((4096, 784),1468618882187.8767 )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2.2. Output vectors: one-hot encoding\n",
    "\n",
    "The labels we have are just numbers from 0-9. We *could* try and build a function that outputs those numbers directly, but it won't work very well. It's much better to encode the labels as vectors ; this will make it much easier to learn the function that maps from inputs to outputs.\n",
    "\n",
    "To do this, we'll use a standard transform called **one-hot encoding**. This means that each label from a set of labels of size N is represented as a vector of length N, with a 1 in the position of the label and 0 elsewhere.\n",
    "\n",
    "For example, the label 3 (dress) in the 10 labels in our dataset would be encoded as the vector `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`. (remember we count from 0).\n",
    "\n",
    "Write two functions:\n",
    "* `to_one_hot(labels, max_d)` that takes a (N,) vector of labels and returns a matrix of one-hot encoded (N, max_d) labels. Max_d is the number of distinct labels (e.g. 10 for the clothes dataset).\n",
    "* `from_one_hot(labels)` that takes a (N, max_d) matrix of one-hot encoded labels and returns a (N,) vector of labels. Note: do not assume that you get a perfect encoding; you should return the label with the *highest* value.\n",
    "\n",
    "Assume that labels are consecutive integers from 0 to `max_d-1`. Note: these functions are trivial to implement in NumPy.\n",
    "\n",
    "Then one-hot encode the `clothes_labels` into `clothes_outputs` (a `(4096,10)` matrix).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22d7adb855c385fdfdb01033d05e5a11",
     "grade": false,
     "grade_id": "cell-1ddabfe9c581d086",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68bbbb370bd179594af93c1dc2e527de",
     "grade": true,
     "grade_id": "cell-f1a933e58947fa38",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tick.marks(4):\n",
    "    assert to_one_hot(np.array([0]), 10).shape == (1, 10)\n",
    "    assert to_one_hot(np.array([0, 1, 2]), 10).dtype == np.float64, \"Make sure to convert your array to floating point with .astype(np.float64)\"\n",
    "    assert to_one_hot(np.array([0, 1, 2]), 10).shape == (3, 10)\n",
    "    assert to_one_hot(np.array([0, 1, 2]), 3).shape == (3, 3)\n",
    "    assert np.allclose(to_one_hot(np.array([0, 1, 2]), 3)[0],[1, 0, 0])\n",
    "    assert np.allclose(to_one_hot(np.array([0, 1, 2]), 3)[1],[0, 1, 0])\n",
    "    assert np.allclose(to_one_hot(np.array([0, 1, 2]), 3)[2],[0, 0, 1])\n",
    "    assert np.allclose(from_one_hot(np.array([[1,0,0], [0,1,0], [0,0,1]])), [0, 1, 2])\n",
    "    assert from_one_hot(np.array([[1,0,0,0]])) == [0]\n",
    "    assert from_one_hot(np.array([[0.9,0,0,0]])) == [0], \"Are you finding the highest value?\"\n",
    "    assert from_one_hot(np.array([[10,0,0,0]])) == [0]\n",
    "    assert from_one_hot(np.array([[0.9,0.9,0.9,1]])) == [3], \"Are you finding the highest value?\"\n",
    "    assert from_one_hot(np.array([[0,0,0,1]])) == [3]    \n",
    "    assert check_hash(clothes_outputs, ((4096, 10), 83888018.4))\n",
    "    rng = np.random.default_rng(12345)\n",
    "    corrupted = clothes_outputs + np.random.uniform(-0.2, 0.2, clothes_outputs.shape)\n",
    "    assert np.all(from_one_hot(corrupted) == clothes_labels), \"Make sure you are finding the highest value\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2.3. Creating theta: flattening and initial guesses\n",
    "\n",
    "To be able to optimise this prediction function in the standard form, we have to package *all* of the things that could vary into a single \"flat\" parameter vector $\\theta$. `nn_predict()` can unpack a list of matrices from a single vector if it is given the right `unflatten` parameter. We can use the `flatten` convenience function to make this easy.\n",
    "\n",
    "    theta, unflatten = flatten(structure_of_matrices)\n",
    "\n",
    "See the example below for how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12345)\n",
    "w1 = rng.uniform(-0.1, 0.1, (784, 100))\n",
    "w2 = rng.uniform(-0.1, 0.1, (100, 10))\n",
    "b1 = np.zeros(100)\n",
    "b2 = np.zeros(10)\n",
    "# list of pairs (matrix, vector), (matrix, vector), ...\n",
    "w_b = [[w1, b1], [w2, b2]]\n",
    "# turn into a flat theta\n",
    "flat, unflatten = flatten(w_b)\n",
    "nn_predict(flat, x=clothes_inputs[0], unflatten=unflatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network architecture\n",
    "The choice of the matrix shapes we use to do the prediction affects how well we will be able to model the transformation. In our example, we know we have 784 dimensional inputs (28x28  images unraveled into flat vectors) and 10D outputs (one-hot encoded labels). The parameters of each \"layer\" will be composed of a matrix (the weights) and a vector (the bias).  Introducing intermediate layers allows the network to learn complicated mappings between different spaces, and not just simple linear transformations.\n",
    "\n",
    "A very simple model might have a mapping like `784 -> 10` with just one single layer. \n",
    "\n",
    "A more complex model might have a mapping with extra layers: `784 -> 64 -> 32 -> 16 -> 10`. We would have weight matrices like:\n",
    "\n",
    "    W[0]        W[1]        W[2]        W[3]\n",
    "    784, 64     64, 32      32, 16      16, 10\n",
    "    \n",
    "(the biases `b` are not shown here, but they are also part of the $\\theta$ vector). This **architecture** maps the 784 input vector to some 64 dimensional space, then to some 32 dimensional space, and so on. Every matrix *has* to have an output dimension which matches the input dimension of the following matrix. The matrices W[0], W[1], W[2] specify how the vector at each layer gets mapped to the next layer. All of these matrices `W[i]` and all of the corresponding biases `b[i]` are flattened into a single vector `theta` for optimisation. \n",
    "\n",
    "The choices of these matrix shapes are not hugely important (I just made the ones above up); but more matrices with more elements means a more flexible function which can learn more complicated things; but will be harder to optimise efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisation\n",
    "We need to set up an initial $\\theta$ vector for our model. We can define the shape of each matrix and vector, but we don't know what the values of the elements in the matrices should be, because we will find this by optimisation. So we just make a random guess.\n",
    "\n",
    "Create a function `initial_conditions` that:\n",
    "* takes a list of layer *shapes* `[(M1,N1), (M2, N2), ...]`, and create a corresponding list of pairs (tuples) of randomly filled matrices and vectors (each M,N matrix should be paired with a length N vector).  \n",
    "* The function should take a parameter `sigma` that should specify the spread of the random values chosen for each layer.\n",
    "* The function should take a random number generator `rng` used to generate random values.\n",
    "\n",
    "Use `rng.normal(0, sigma, shape)` to generate the random numbers.\n",
    "\n",
    "Return the **flattened** version of the matrix list, along with the corresponding `unflatten` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c415dc8a85e638a54c837e9afa8953f1",
     "grade": false,
     "grade_id": "cell-f301a7973bd8d6db",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initial_conditions(shape_list, sigma_list, rng):    \n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15ee95cff7ca8f13a9b0610a583ed117",
     "grade": true,
     "grade_id": "cell-c22869a7e904b084",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tick.marks(4):\n",
    "    rng = np.random.default_rng(12345)\n",
    "    theta, unflatten = initial_conditions([[8,4], [4,8], [4,2]], [0.1]*3, rng)\n",
    "    assert callable(unflatten), \"You should return the unflatten function that flatten gives you.\"\n",
    "    matrices = unflatten(theta)\n",
    "    assert len(matrices)==3\n",
    "    assert len(matrices[0])==2, \"You need to return a matrix, vector pair for each layer\"\n",
    "    assert matrices[0][0].shape[1]==matrices[0][1].shape[0], \"The shapes of the matrices need to match the vectors\"\n",
    "    assert(matrices[0][0].shape==(8,4))\n",
    "    assert(matrices[0][1].shape==(4,))  \n",
    "    assert(matrices[1][0].shape==(4,8))\n",
    "    assert(matrices[1][1].shape==(8,))\n",
    "    assert(matrices[2][0].shape==(4,2))\n",
    "    assert(matrices[2][1].shape==(2,))\n",
    "    assert np.allclose(matrices[0][0][0][0], -0.14238250364546312), \"Did you use rng.normal?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random predictions\n",
    "We can now use `nn_predict` to see the effect of applying a random initial condition function to the clothes. Since all of the matrices are random, the result will be a random mess of very poorly sorted washing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "177c210b0a50b1440ea0fe6182c8cb3c",
     "grade": false,
     "grade_id": "cell-2d19ab914fa42af3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create some test initial conditions\n",
    "rng = np.random.default_rng(1998)\n",
    "n = 256\n",
    "random_theta, unflatten = initial_conditions( [[784, 10]], [1, 1, 1, 1], rng)\n",
    "\n",
    "# predict the outputs (will be random junk)\n",
    "predicted_outputs = [nn_predict(random_theta, clothes_inputs[i], unflatten) for i in range(n)]\n",
    "predicted_labels = from_one_hot(predicted_outputs)\n",
    "\n",
    "def show_encoded_clothes(images, predicted_labels):\n",
    "    red = np.array([1, 0, 0])\n",
    "    green = np.array([0, 1, 0])\n",
    "    fig = plt.figure(dpi=100, figsize=(12, 8))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.set_facecolor('k')    \n",
    "    racks = [0] * 10     \n",
    "    for i in range(images.shape[0]): \n",
    "        x = racks[predicted_labels[i]]\n",
    "        y = predicted_labels[i]\n",
    "        racks[predicted_labels[i]] += 1        \n",
    "        if predicted_labels[i] == clothes_labels[i]:\n",
    "            img = images[i].reshape(28,28)[:,:,None] * green        \n",
    "        else:\n",
    "            img = images[i].reshape(28,28)[:,:,None] * red\n",
    "        ax.imshow(img, extent=[x, x+1, y, y+1])\n",
    "    \n",
    "\n",
    "    ax.set_xlim(0, 80)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.set_yticks(np.arange(10)+0.5)\n",
    "    ax.set_yticklabels(label_names)\n",
    "    ax.set_title(\"Encoded clothes\")\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Category')\n",
    "    fig.set_facecolor('black')\n",
    "    # set ticks/axis labels to white\n",
    "    ax.xaxis.label.set_color('white')\n",
    "    ax.yaxis.label.set_color('white')        \n",
    "    ax.tick_params(axis='y', colors='white')\n",
    "    # set the tick labels to white\n",
    "    for label in ax.get_yticklabels():\n",
    "        label.set_color('white')\n",
    "    # set title to white\n",
    "    ax.title.set_color('white')\n",
    "    \n",
    "    \n",
    "show_encoded_clothes(clothes_inputs[:n], np.array(predicted_labels)[:n])\n",
    "plt.gca().set_title(\"Random predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.3 Objective function\n",
    "\n",
    "### B.3.1 Per-vector loss\n",
    "Write an objective function that will compare the predicted output to the actual output, given `theta`, `unflatten`, and `x` and `y`. Use $L_2$ norm as the loss function. The loss function will need to call `nn_predict` to calculate `y_prime`, the predicted output to compare with `y`. Assume `x` and `y` are vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08e49a5f5877d9a65b6b6db55f46202b",
     "grade": false,
     "grade_id": "cell-305c064d9cca8eed",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def nn_loss(theta, x, y, unflatten):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39163352563c261702d339267c338827",
     "grade": true,
     "grade_id": "cell-abd9b2a75fa74b9c",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# create a random set of weights, and test that the loss function is computed correctly\n",
    "\n",
    "\n",
    "with tick.marks(4):    \n",
    "    theta, unflatten = flatten([])\n",
    "    # check that the identity holds\n",
    "    assert nn_loss(theta, np.zeros(784), np.zeros(784), unflatten) == 0.0\n",
    "    assert nn_loss(theta, clothes_inputs[5],clothes_inputs[5], unflatten) == 0.0\n",
    "    assert nn_loss(theta, np.zeros(784), clothes_inputs[5], unflatten) == np.linalg.norm(clothes_inputs[5]), \"Are you using the L2 norm?\"\n",
    "\n",
    "    # check that doing random things gives a non-zero result\n",
    "    rng = np.random.default_rng(12345)\n",
    "    theta, unflatten = flatten([\n",
    "        (rng.normal(0,1,(784, 100)), rng.normal(0,1,100)),\n",
    "        (rng.normal(0,1,(100, 784)), rng.normal(0,1,784))\n",
    "        ])\n",
    "    assert nn_loss(theta, np.zeros(784), np.zeros(784), unflatten) > 0.0\n",
    "    # check that different shapes for input and output work okay\n",
    "    theta, unflatten = flatten([\n",
    "        (rng.normal(0,1,(784, 100)), rng.normal(0,1,100)),        \n",
    "        ])\n",
    "    assert nn_loss(theta, np.zeros(784), np.zeros(100), unflatten) > 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.3.2 Total loss\n",
    "Write a function `reconstruction_loss` that computes the sum of the objective function value for *every* input in `clothes_inputs`.  That is, it computes:\n",
    "\n",
    "$$L(\\theta) = \\sum_i \\|y_i - f(\\vec{x_i};\\theta)\\|$$\n",
    "\n",
    "where $x_i$ is the ith row of `clothes_inputs`, $y_i$ is the corresponding row of `clothes_output` and `theta` is the parameter vector. The function must also take an `unflatten` argument so it can unpack the parameter vector correctly. **This function should use the `loss` function you defined above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e232c2abb4e1f578b9200e9d4a865ea",
     "grade": false,
     "grade_id": "cell-a52fc79c200b3f58",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# compute the sum of losses\n",
    "# for every pair of xs and ys\n",
    "def reconstruction_loss(theta, unflatten):\n",
    "    # YOUR CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "396199c7cb5644cb16e8567e92f49af3",
     "grade": true,
     "grade_id": "cell-cbef7ebe8bc09215",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "rng_test = np.random.default_rng(2000)\n",
    "\n",
    "# create some test initial conditions\n",
    "random_theta, random_unflatten = flatten(\n",
    "    [(rng_test.normal(0,1,(784,10)), rng_test.normal(0,1,(10,)))])\n",
    "\n",
    "with tick.marks(2):        \n",
    "    print(reconstruction_loss(random_theta, random_unflatten))\n",
    "    assert(np.allclose(reconstruction_loss(random_theta, random_unflatten), 84246.65556971483 , atol=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B.4 Optimisation\n",
    "\n",
    "### Bad: hillclimbing\n",
    "We could try learning the `reconstruction_loss` objective function with hill climbing, but we'll find we have very poor performance and it is *extremely* slow even with a very simple network. Try the code below; it will take about 30-60 seconds to do something, and while the result is better than random it is not good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat, unflatten = initial_conditions([[784, 10]], [0.1], rng)\n",
    "make_guess = lambda: initial_conditions([[784, 10]], [0.1], rng)[0]                                        \n",
    "t_loss = lambda theta:reconstruction_loss(theta, unflatten)\n",
    "with no_graph():\n",
    "    best_theta = hill_climbing(t_loss, 50, make_guess, get_proposal_fn(0.01, rng))\n",
    "print(t_loss(best_theta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65a869d04a09221627502e7c07003350",
     "grade": false,
     "grade_id": "cell-6c675faa75adb168",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chaotically sorted laundry\n",
    "def show_predictions(theta, unflatten, n, show=True):\n",
    "    predicted_outputs = [nn_predict(theta, clothes_inputs[i], unflatten) for i in range(n)]\n",
    "    predicted_labels = from_one_hot(predicted_outputs)\n",
    "    if show:\n",
    "        show_encoded_clothes(clothes_inputs[:n], np.array(predicted_labels)[:n])\n",
    "        plt.gca().set_title(\"Sorted clothes\")\n",
    "        plt.show()\n",
    "    pct_correct = np.mean(predicted_labels == clothes_labels[:n])\n",
    "    print(f\"Correctly sorted {pct_correct:.1%} of the clothes\")\n",
    "    \n",
    "    return pct_correct\n",
    "\n",
    "show_predictions(best_theta, unflatten, n=1024)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### B.4.1. Good: Stochastic gradient descent\n",
    "Now we will optimise with stochastic gradient descent, and get a much better result.  In this task, you will have to define a function `sgd_learn`. \n",
    "\n",
    "`sgd_learn(obj_fun, shapes, inputs, outputs, sigmas, rng, step, iters)` should:\n",
    "* take an objective function\n",
    "* and a list of matrix `shapes`, a list of `sigmas` to specify the random initialisation of those matrices, a `rng` random number generator, a `step` size (delta) and a number of `iterations`\n",
    "* generate an initial `theta` from that set of shapes, using the `initial_condition()` function you defined above.\n",
    "* for each of the given number of iterations\n",
    "    * randomly select *ONE* input vector (from `clothes_inputs`) and matching output vector (from `clothes_outputs`).\n",
    "    * compute the gradient of the objective function for that image/output pair\n",
    "    * make a step, adjusting `theta` in the direction of this gradient, scaled by the step size\n",
    "\n",
    "* return the flattened theta vector and the corresponding unflatten function\n",
    "\n",
    "You will need to compute the gradient of the objective function using `grad`. Remember that `grad(f)` takes a function `f` and returns **a new function** that computes the gradient of `f` with respect to its first argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You don't need to implement *any* sophistications like momentum or random restart. You don't need to collect the data into minibatches. The algorithm you implement should be very simple. (you can add these things if you want, but it is not necessary to pass all of the tests).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7e7953a12ab18c9c32c3f4e84096333",
     "grade": false,
     "grade_id": "cell-1548bffbe7a4d279",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "\n",
    "def sgd_learn(obj_fun, shapes, inputs, outputs, sigmas, rng, step=0.1, iters=10000):\n",
    "    c = Convergence(interval=100, graph=True)\n",
    "    # YOUR CODE HERE\n",
    "    c.close()\n",
    "    return w, unflatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "722155f2783d96524dedebaea28bd6fe",
     "grade": true,
     "grade_id": "cell-c5e1dcc9c57d02d0",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# verify that the shapes come out right\n",
    "with tick.marks(2):\n",
    "    with no_graph():\n",
    "        test_theta, test_unflatten = sgd_learn(nn_loss, [[784,10]], clothes_inputs, np.ones(len(clothes_inputs)), [0.1]*3, rng, 0.1, 1)\n",
    "    unflattened = test_unflatten(test_theta)\n",
    "    assert(len(unflattened)==1)\n",
    "    assert((unflattened[0][0].shape)==(784,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75dc12511d573ec71c2135a4c88550ed",
     "grade": true,
     "grade_id": "cell-53b4afa114ae3fd2",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## verify that some learning happens\n",
    "rng_test = np.random.default_rng(2024)\n",
    "with tick.marks(4):\n",
    "    with no_graph():\n",
    "        xs = clothes_inputs\n",
    "        ys = np.ones(len(clothes_inputs))\n",
    "        test_theta, test_unflatten = sgd_learn(nn_loss, [[784,1]], xs, ys, [0.1]*3, rng_test, 0.1, 1)\n",
    "        before_losses = [nn_loss(test_theta, x,y, test_unflatten) for x,y in zip(xs, ys)]\n",
    "        \n",
    "        test_theta, test_unflatten = sgd_learn(nn_loss, [[784,1]], xs, ys, [0.1], rng_test, 0.05, 1000)\n",
    "        after_losses = [nn_loss(test_theta, x,y, test_unflatten) for x,y in zip(xs, ys)]\n",
    "        print(\"Mean loss before optimising %.2f; after optimising %.2f\" % (np.mean(before_losses),  np.mean(after_losses)))\n",
    "        assert(np.mean(after_losses)/np.mean(before_losses)<0.5)\n",
    "        print(\"Something was learned!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.4.2 Sort the washing\n",
    "\n",
    "Use this function to learn the mapping function from clothes images to their label vectors.\n",
    "\n",
    "* a **step size** (values in the range 0.2 to 0.0001 are reasonable)\n",
    "* the **sigmas** for the initial conditions (values in the range 0.5 to 0.005 are reasonable).\n",
    "\n",
    "These are **hyperparameters** of the optimisation process. \n",
    "\n",
    "* You should use *no more* than 50_000 iterations in the learning process. \n",
    "\n",
    "You should use a smaller number of iterations while you are establishing your hyperparameters, and then run the full number of iterations once you have a good idea of the best hyperparameters.\n",
    "\n",
    "**Warning: if your call to `sgd_learn` takes much more than ten minutes to run, the autograder will not accept your result!**\n",
    "\n",
    "Use matrix shapes `[[784,64], [64, 32],  [32,10]`, *or* choose your own set of matrix shapes (just don't make them so large the optimisation takes forever).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98ed189fbac4599b803177413bcf5598",
     "grade": false,
     "grade_id": "cell-9fed416acb37ebd2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "## Run sgd_learn(...) in this cell\n",
    "## produce the output theta, unflatten\n",
    "## theta, unflatten = learn(...\n",
    "## LEAVE THIS HERE TO FORCE CONSISTENT RESULTS!\n",
    "rng = np.random.default_rng(2024)\n",
    "plt.show()\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00f2385c446e573f6180006e64deb91a",
     "grade": false,
     "grade_id": "cell-d1b10b0a8e4a1cfc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's sort the washing!\n",
    "pct_correct = show_predictions(theta, unflatten, n=1024)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab5a868fc49fce230d459d7162aa778e",
     "grade": true,
     "grade_id": "cell-4471cd37fa7303256",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# you get more marks for a more correct answer :)\n",
    "with tick.marks(1):\n",
    "    pct_correct = show_predictions(theta, unflatten, n=1024, show=False)    \n",
    "    assert pct_correct>0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f66adab43a4d7ec1e45db01b821ce88",
     "grade": true,
     "grade_id": "cell-fd818f625a6301b9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tick.marks(2):\n",
    "    pct_correct = show_predictions(theta, unflatten, n=1024, show=False)    \n",
    "    assert pct_correct>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d49d1de9a954b85139bf4b5bbe308d00",
     "grade": true,
     "grade_id": "cell-fef03547993b6f12",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "with tick.marks(4):\n",
    "    pct_correct = show_predictions(theta, unflatten, n=1024, show=False)    \n",
    "    assert pct_correct>0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06d64c530ec444d50a4ef3eb93a51dab",
     "grade": true,
     "grade_id": "cell-3e8c42fe8ea2caae",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "with tick.marks(2):\n",
    "    pct_correct = show_predictions(theta, unflatten, n=1024, show=False)    \n",
    "    assert pct_correct>0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've got this far, you've managed to build a system which can predict sort your washing. Well, sort of. As long as you have perfectly aligned images without any distracting background. All that was needed was some simple gradient descent, which optimised a function parameterised with a single high-dimensional vector (the number of elements in `theta`).\n",
    "\n",
    "> Note: we didn't really test this fairly; we just tested if we could recognise the images we had trained on. A real system would test if the predictions worked on images that were not in the training set, as we may have learned irrelevant features of the training set (\"overfitting\"). You can test this if you want: there's a corresponding `data/fashion_test.png` and `data/fashion_test_labels.txt` file you could use to test the system. But you don't get any marks for this!\n",
    "\n",
    "# END OF LAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33c82f94f9b715a9d467498cf8b0b6cb",
     "grade": true,
     "grade_id": "cell-e7b4835eca355c47",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prestige marks only! This does not affect your final mark\n",
    "# if you want you can try and get this\n",
    "# you will need to do more than just adjust hyperparameters to do this\n",
    "# (e.g. change architecture, add real batching, etc.)\n",
    "with tick.marks(0):\n",
    "    pct_correct = show_predictions(theta, unflatten, n=1024, show=False)    \n",
    "    assert pct_correct>0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Submission instructions (for assessed labs only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before submission\n",
    "\n",
    "* Make sure you fill in any place that says `YOUR CODE HERE` or `\"YOUR ANSWER HERE\"`.\n",
    "* SAVE THE NOTEBOOK\n",
    "* DO NOT RENAME THE NOTEBOOK OR IT WILL NOT BE MARKED."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    \n",
    "### Formatting the submission\n",
    "* **WARNING**: If you do not submit the correct file, you will not get any marks.\n",
    "* Submit this file **only** on Moodle. It will be named `<xxx>.ipynb`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Penalties (only for assessed labs)\n",
    "<font color=\"red\">\n",
    "    \n",
    "**Malformatted submissions**\n",
    "</font>\n",
    "These assignments are processed with an automatic tool; failure to follow instructions *precisely* will lead to you automatically losing two bands in grade regardless of whether the work is correct (not to mention a long delay in getting your work back). **If you submit a file without your work in it, it will be marked and you will get 0 marks.**\n",
    "\n",
    "<font color=\"red\">**Late submission**</font>\n",
    "Be aware that there is a two band penalty for every *day* of late submission, starting the moment of the deadline.\n",
    "\n",
    "<font color=\"red\">\n",
    "    \n",
    "**Plagiarism**\n",
    "</font> Plagiarism will be subject to the Plagiarism Policy. The penalties are severe."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "format_version": "1.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
